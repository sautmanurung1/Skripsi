{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import pairwise_distances\n",
    "# from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# # Load data from CSV file\n",
    "# data = pd.read_csv('Data Process.csv', sep=';')\n",
    "\n",
    "# # Exclude the first data row\n",
    "# data = data.iloc[1:]\n",
    "\n",
    "# # Replace commas (,) with periods (.) and convert to float\n",
    "# data['Peng Sem 1'] = data['Peng Sem 1'].str.replace(',', '.').astype(float)\n",
    "# data['Peng Sem 2'] = data['Peng Sem 2'].str.replace(',', '.').astype(float)\n",
    "# data['Ket Sem 1'] = data['Ket Sem 1'].str.replace(',', '.').astype(float)\n",
    "# data['Ket Sem 2'] = data['Ket Sem 2'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# # Extract attributes from the data\n",
    "# attributes = ['Nama', 'Peng Sem 1', 'Ket Sem 1', 'Peng Sem 2', 'Ket Sem 2']\n",
    "# X = data[attributes].values\n",
    "\n",
    "# # Perform pairwise distance calculation\n",
    "# distances = pairwise_distances(X[:, 1:], metric='euclidean')\n",
    "\n",
    "# # Function to calculate the total dissimilarity for a given medoid index\n",
    "# def total_dissimilarity(index, medoids):\n",
    "#     cluster_indices = np.where(medoids)[0]\n",
    "#     cluster_points = X[cluster_indices]\n",
    "#     cluster_distances = distances[cluster_indices][:, cluster_indices]\n",
    "#     return sum(cluster_distances)\n",
    "\n",
    "# # Function to find the best medoid with the lowest dissimilarity\n",
    "# def find_best_medoid(cluster_points, cluster_indices):\n",
    "#     best_medoid = None\n",
    "#     best_dissimilarity = float('inf')\n",
    "#     for i in range(len(cluster_points)):\n",
    "#         dissimilarity = total_dissimilarity(i, cluster_indices)\n",
    "#         if np.all(dissimilarity < best_dissimilarity):\n",
    "#             best_medoid = cluster_indices[i]  # Update with cluster index\n",
    "#             best_dissimilarity = dissimilarity\n",
    "#     return best_medoid, best_dissimilarity\n",
    "\n",
    "# # Perform K-Medoids clustering\n",
    "# k = 3  # Number of clusters\n",
    "# medoids_indices = KMedoids(n_clusters=k, random_state=0).fit_predict(distances)\n",
    "\n",
    "# # Find the best medoid for each cluster\n",
    "# medoids = []\n",
    "# for cluster_id in range(k):\n",
    "#     cluster_indices = np.where(medoids_indices == cluster_id)[0]\n",
    "#     cluster_points = X[cluster_indices]\n",
    "#     medoid_index, _ = find_best_medoid(cluster_points, cluster_indices)\n",
    "#     medoids.append(medoid_index)\n",
    "\n",
    "# # Retrieve all attributes from data\n",
    "# all_attributes = X\n",
    "\n",
    "# # Print all the attributes along with the cluster assignment (starting from 1)\n",
    "# for i, row in enumerate(all_attributes):\n",
    "#     cluster_assignment = medoids_indices[i] + 1\n",
    "#     print(f\"Nama: {row[0]}, Peng Sem 1: {row[1]}, Ket Sem 1: {row[2]}, Peng Sem 2: {row[3]}, Ket Sem 2: {row[4]}, Cluster: {cluster_assignment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data Process.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18740/1984296143.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Load data from CSV file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data Process.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Exclude the first data row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\NITRO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\NITRO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 )\n\u001b[1;32m--> 331\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\NITRO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\NITRO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\NITRO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\NITRO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1735\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\NITRO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data Process.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv('Data Process.csv', sep=';')\n",
    "\n",
    "# Exclude the first data row\n",
    "data = data.iloc[1:]\n",
    "\n",
    "# Replace commas (,) with periods (.) and convert to float\n",
    "data['Peng Sem 1'] = data['Peng Sem 1'].str.replace(',', '.').astype(float)\n",
    "data['Peng Sem 2'] = data['Peng Sem 2'].str.replace(',', '.').astype(float)\n",
    "data['Ket Sem 1'] = data['Ket Sem 1'].str.replace(',', '.').astype(float)\n",
    "data['Ket Sem 2'] = data['Ket Sem 2'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Extract attributes from the data\n",
    "attributes = ['Nama', 'Peng Sem 1', 'Ket Sem 1', 'Peng Sem 2', 'Ket Sem 2']\n",
    "X = data[attributes].values\n",
    "\n",
    "# Perform pairwise distance calculation\n",
    "distances = pairwise_distances(X[:, 1:], metric='euclidean')\n",
    "\n",
    "# Function to calculate the total dissimilarity for a given medoid index\n",
    "def total_dissimilarity(index, medoids):\n",
    "    cluster_indices = np.where(medoids)[0]\n",
    "    cluster_points = X[cluster_indices]\n",
    "    cluster_distances = distances[cluster_indices][:, cluster_indices]\n",
    "    return sum(cluster_distances)\n",
    "\n",
    "# Function to find the best medoid with the lowest dissimilarity\n",
    "def find_best_medoid(cluster_points, cluster_indices):\n",
    "    best_medoid = None\n",
    "    best_dissimilarity = float('inf')\n",
    "    for i in range(len(cluster_points)):\n",
    "        dissimilarity = total_dissimilarity(i, cluster_indices)\n",
    "        if np.all(dissimilarity < best_dissimilarity):\n",
    "            best_medoid = cluster_indices[i]  # Update with cluster index\n",
    "            best_dissimilarity = dissimilarity\n",
    "    return best_medoid, best_dissimilarity\n",
    "\n",
    "# Perform K-Medoids clustering\n",
    "k = 3  # Number of clusters\n",
    "medoids_indices = KMedoids(n_clusters=k, random_state=0).fit_predict(distances)\n",
    "\n",
    "# Find the best medoid for each cluster\n",
    "medoids = []\n",
    "for cluster_id in range(k):\n",
    "    cluster_indices = np.where(medoids_indices == cluster_id)[0]\n",
    "    cluster_points = X[cluster_indices]\n",
    "    medoid_index, _ = find_best_medoid(cluster_points, cluster_indices)\n",
    "    medoids.append(medoid_index)\n",
    "\n",
    "# Retrieve all attributes from data\n",
    "all_attributes = X\n",
    "\n",
    "# Create a new DataFrame with cluster assignments\n",
    "cluster_data = pd.DataFrame(all_attributes, columns=attributes)\n",
    "cluster_data['Cluster'] = medoids_indices + 1\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "cluster_data.to_csv('Clustered_Data.csv', index=False)\n",
    "cluster_counts = cluster_data['Cluster'].value_counts()\n",
    "print(cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import pairwise_distances\n",
    "# from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# # Load data from CSV file\n",
    "# data = pd.read_csv('Datas_Nilai.csv', sep=';')\n",
    "\n",
    "# # Exclude the first data row\n",
    "# data = data.iloc[1:]\n",
    "\n",
    "# # Remove rows with missing values\n",
    "# data = data.dropna()\n",
    "\n",
    "# # Replace commas (,) with periods (.) and convert to float\n",
    "# data['Peng Sem 1'] = data['Peng Sem 1'].str.replace(',', '.').astype(float)\n",
    "# data['Peng Sem 2'] = data['Peng Sem 2'].str.replace(',', '.').astype(float)\n",
    "# data['Ket Sem 1'] = data['Ket Sem 1'].str.replace(',', '.').astype(float)\n",
    "# data['Ket Sem 2'] = data['Ket Sem 2'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# # Extract attributes from the data\n",
    "# attributes = ['Nama', 'Peng Sem 1', 'Ket Sem 1', 'Peng Sem 2', 'Ket Sem 2', 'Kelas Awal']\n",
    "# X = data[attributes].values\n",
    "\n",
    "# # Perform pairwise distance calculation\n",
    "# distances = pairwise_distances(X[:, 1:5], metric='euclidean')\n",
    "\n",
    "# # Function to calculate the total dissimilarity for a given medoid index\n",
    "# def total_dissimilarity(index, medoids):\n",
    "#     cluster_indices = np.where(medoids)[0]\n",
    "#     cluster_points = X[cluster_indices]\n",
    "#     cluster_distances = distances[cluster_indices][:, cluster_indices]\n",
    "#     return sum(cluster_distances)\n",
    "\n",
    "# # Function to find the best medoid with the lowest dissimilarity\n",
    "# def find_best_medoid(cluster_points, cluster_indices):\n",
    "#     best_medoid = None\n",
    "#     best_dissimilarity = float('inf')\n",
    "#     for i in range(len(cluster_points)):\n",
    "#         dissimilarity = total_dissimilarity(i, cluster_indices)\n",
    "#         if np.all(dissimilarity < best_dissimilarity):\n",
    "#             best_medoid = cluster_indices[i]  # Update with cluster index\n",
    "#             best_dissimilarity = dissimilarity\n",
    "#     return best_medoid, best_dissimilarity\n",
    "\n",
    "# # Perform K-Medoids clustering\n",
    "# k = 3  # Number of clusters\n",
    "# medoids_indices = KMedoids(n_clusters=k, random_state=0).fit_predict(distances)\n",
    "\n",
    "# # Find the best medoid for each cluster\n",
    "# medoids = []\n",
    "# for cluster_id in range(k):\n",
    "#     cluster_indices = np.where(medoids_indices == cluster_id)[0]\n",
    "#     cluster_points = X[cluster_indices]\n",
    "#     medoid_index, _ = find_best_medoid(cluster_points, cluster_indices)\n",
    "#     medoids.append(medoid_index)\n",
    "\n",
    "# # Retrieve all attributes from data\n",
    "# all_attributes = X\n",
    "\n",
    "# # Create a dictionary to map cluster assignments to new classes\n",
    "# cluster_to_class = {\n",
    "#     0: '7a',\n",
    "#     1: '7b',\n",
    "#     2: '7c',\n",
    "# }\n",
    "\n",
    "# # Append new column 'Kelas Baru' to the DataFrame\n",
    "# data['Kelas Baru'] = ''\n",
    "\n",
    "# # Update the 'Kelas Baru' column with the new class based on the cluster assignment\n",
    "# for i, row in enumerate(all_attributes):\n",
    "#     cluster_assignment = medoids_indices[i]\n",
    "#     new_class = cluster_to_class[cluster_assignment]\n",
    "#     data.loc[i, 'Kelas Baru'] = new_class\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import pairwise_distances\n",
    "# from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# # Load data from CSV file\n",
    "# data = pd.read_csv('Data Process.csv', sep=';')\n",
    "\n",
    "# # Exclude the first data row\n",
    "# data = data.iloc[1:]\n",
    "\n",
    "# # Remove rows with missing values\n",
    "# data = data.dropna()\n",
    "\n",
    "# # Replace commas (,) with periods (.) and convert to float\n",
    "# data['Peng Sem 1'] = data['Peng Sem 1'].str.replace(',', '.').astype(float)\n",
    "# data['Peng Sem 2'] = data['Peng Sem 2'].str.replace(',', '.').astype(float)\n",
    "# data['Ket Sem 1'] = data['Ket Sem 1'].str.replace(',', '.').astype(float)\n",
    "# data['Ket Sem 2'] = data['Ket Sem 2'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# # Extract attributes from the data\n",
    "# attributes = ['Nama', 'Peng Sem 1', 'Ket Sem 1', 'Peng Sem 2', 'Ket Sem 2', 'Kelas Awal']\n",
    "# X = data[attributes].values\n",
    "\n",
    "# # Perform pairwise distance calculation\n",
    "# distances = pairwise_distances(X[:, 1:5], metric='euclidean')\n",
    "\n",
    "# # Function to calculate the total dissimilarity for a given medoid index\n",
    "# def total_dissimilarity(index, medoids):\n",
    "#     cluster_indices = np.where(medoids)[0]\n",
    "#     cluster_points = X[cluster_indices]\n",
    "#     cluster_distances = distances[cluster_indices][:, cluster_indices]\n",
    "#     return sum(cluster_distances)\n",
    "\n",
    "# # Function to find the best medoid with the lowest dissimilarity\n",
    "# def find_best_medoid(cluster_points, cluster_indices):\n",
    "#     best_medoid = None\n",
    "#     best_dissimilarity = float('inf')\n",
    "#     for i in range(len(cluster_points)):\n",
    "#         dissimilarity = total_dissimilarity(i, cluster_indices)\n",
    "#         if np.all(dissimilarity < best_dissimilarity):\n",
    "#             best_medoid = cluster_indices[i]  # Update with cluster index\n",
    "#             best_dissimilarity = dissimilarity\n",
    "#     return best_medoid, best_dissimilarity\n",
    "\n",
    "# # Perform K-Medoids clustering\n",
    "# k = 8 # Number of clusters\n",
    "# medoids_indices = KMedoids(n_clusters=k, random_state=0).fit_predict(distances)\n",
    "\n",
    "# # Find the best medoid for each cluster\n",
    "# medoids = []\n",
    "# for cluster_id in range(k):\n",
    "#     cluster_indices = np.where(medoids_indices == cluster_id)[0]\n",
    "#     cluster_points = X[cluster_indices]\n",
    "#     medoid_index, _ = find_best_medoid(cluster_points, cluster_indices)\n",
    "#     medoids.append(medoid_index)\n",
    "\n",
    "# # Retrieve all attributes from data\n",
    "# all_attributes = X\n",
    "\n",
    "# # Create a dictionary to map cluster assignments to new classes\n",
    "# cluster_to_class = {\n",
    "#     0: '7a',\n",
    "#     1: '7b',\n",
    "#     2: '7c',\n",
    "#     3: '7d',\n",
    "#     4: '7e',\n",
    "#     5: '7f',\n",
    "#     6: '7g',\n",
    "#     7: '7h'\n",
    "# }\n",
    "\n",
    "# # Count data points in each cluster\n",
    "# cluster_counts = np.bincount(medoids_indices)\n",
    "\n",
    "# # Append new column 'Kelas Baru' to the DataFrame\n",
    "# data['Kelas Baru'] = ''\n",
    "\n",
    "# # Update the 'Kelas Baru' column with the new class based on the cluster assignment\n",
    "# for i, row in enumerate(all_attributes):\n",
    "#     cluster_assignment = medoids_indices[i]\n",
    "#     new_class = cluster_to_class[cluster_assignment]\n",
    "#     data.loc[i, 'Kelas Baru'] = new_class\n",
    "\n",
    "# # Print the count of data points in each cluster\n",
    "# for cluster_id, count in enumerate(cluster_counts):\n",
    "#     print(f\"Cluster {cluster_id + 1} count: {count}\")\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
